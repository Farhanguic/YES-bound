# -*- coding: utf-8 -*-
"""YES_bound_paper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Vokk3hOq40sVjrRv7tx27CLmjTgBb7z
"""

import numpy as np
import torch as tc
import torch.nn as nn
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
import torch.optim.lr_scheduler as lr_scheduler
from torch.optim.lr_scheduler import MultiStepLR
import torch.nn.functional as F
from google.colab import drive
import pickle
import copy
import torch.nn.init as init
from IPython.display import clear_output
import time
# Mount Google Drive
drive.mount('/content/gdrive')

class struct():
    '''
    an empty class to use structure type variable
    '''
    pass

class Fully_model(tc.nn.Module):
    def __init__(self,params):
      super(Fully_model, self).__init__()
      self.K=params.Layers
      m=params.m
      n=params.n
      Layers=params.Layers
      self.W_1=nn.Linear(n,m,bias=params.bias)
      self.W=nn.ModuleList([nn.Linear(m,m,bias=params.bias) for _ in range(Layers-1)])
############################################################################################################################################################
    def forward(self,x):
      per_out=[]
      x_k=x.to(tc.float32)
      x_k=tc.nn.functional.relu(self.W_1(x_k))
      per_out.append(x_k)
      for k in range(self.K-1):
        temp=self.W[k](x_k)
        #x_k=tc.nn.functional.relu(temp)
        if k != self.K-2:
          x_k=tc.nn.functional.relu(temp)
        else:
          x_k=temp
        per_out.append(x_k)
      return  x_k,per_out
############################################################################################################################################################
def  prepare_train_test(params):
    x_train=params.x_train
    x_test=params.x_test
    BATCH_SIZE=params.BATCH_SIZE
    b_train=params.b_train
    b_test=params.b_test
    # convert data to  iterables
    trainset = []
    for i in range(len(x_train)):
      trainset.append([x_train[i], b_train[i]])
    train_loader= DataLoader(trainset, shuffle=False, batch_size=BATCH_SIZE)
    testset = []
    for i in range(len(x_test)):
      testset.append([x_test[i], b_test[i]])
    test_loader= DataLoader(testset, shuffle=False, batch_size=BATCH_SIZE)
    return train_loader,test_loader
############################################################################################################################################################
def train(model,params,train_loader,test_loader):
   whole_dataset_size=params.whole_dataset_size
   split=params.split
   Num_train=(1-split)*whole_dataset_size
   Num_train=int(Num_train)
   Num_test=(split)*whole_dataset_size
   Num_test=int(Num_test)
   m=params.m
   BATCH_SIZE=params.BATCH_SIZE
   NUM_EPOCHS=params.NUM_EPOCHS
   Layers=params.Layers
   X=params.Xdata
   Y=params.Ydata
   bias_cond=params.bias
   YES_0_layer=[]
   # computing YES-0 bound for training
   if not bias_cond:
    # with no bias
    W_k=tc.matmul(Y,tc.linalg.pinv(X))
    Y_k=tc.nn.functional.relu(tc.matmul(W_k,X))
    YES_0_layer.append(Y_k)
    for k in range(Layers-1):
      W_k=tc.matmul(Y,tc.linalg.pinv(Y_k))
      if k != Layers-2:
        Y_k=tc.nn.functional.relu(tc.matmul(W_k,Y_k))
        YES_0_layer.append(Y_k)
      else:
        Y_k=tc.matmul(W_k,Y_k)
        #Y_k=tc.nn.functional.relu(tc.matmul(W_k,Y_k))
        YES_0_layer.append(Y_k)
   else:
    # with bias
    Y_k=X
    for k in range(Layers):
      Y_t=tc.vstack((Y_k, tc.ones(1,Num_train)))
      W_k=tc.matmul(Y,tc.linalg.pinv(Y_t))
      if k != Layers-1:
        Y_k=tc.nn.functional.relu(tc.matmul(W_k,Y_t))
        YES_0_layer.append(Y_k)
      else:
        Y_k=tc.matmul(W_k,Y_t)
        #Y_k=tc.nn.functional.relu(tc.matmul(W_k,Y_t))
        YES_0_layer.append(Y_k)
   s=0
   for i in range(Num_train):
    s+=(tc.norm(Y_k[:,i]-Y[:,i])**2)
   s=s/Num_train
   YES_0_bound=tc.tensor([s])
   #report=struct()
   criterion=tc.nn.MSELoss(reduction='sum') # square error loss
   optimizer=tc.optim.Adam(model.parameters(),lr=params.lr)
   scheduler=lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.9)
   train_loss_ep=[]
   test_loss_ep=[]
   YES_k_bounds=tc.zeros((Layers-1,NUM_EPOCHS), dtype = tc.float32)
   # Define colors
   spring_green = "#00FF7F"  # Bright and vibrant green
   golden_color = "#FFD700"   # Golden color for the yellow area
   pleasant_red = "#FFB6C1"   # Light pinkish red for the red area
   # determine whether bias of l
   bias_cond=params.bias
   for epoch in range(NUM_EPOCHS):
      for batch in train_loader:
           x,b=batch
           x=x.to(tc.float32)
           b=b.to(tc.float32)
           # forward pass
           b_hat,_=model(x)
           loss=criterion(b_hat,b) # our objective function
           # backward pass
           optimizer.zero_grad()
           loss.backward()
           # update
           optimizer.step()
      scheduler.step()
      current_lr = optimizer.param_groups[0]['lr']
      print(f"Epoch {epoch}, Learning Rate: {current_lr}")
      # creating the per-layer output
      with tc.no_grad():
        All_out=[]
        for i in range(Layers):
          wholeData=tc.zeros((Num_train,m), dtype = tc.float32)
          j=0
          for batch in train_loader:
            x,_=batch
            x=x.to(tc.float32)
            _,per_out=model(x)
            wholeData[BATCH_SIZE*j:BATCH_SIZE*(j+1),:]=per_out[i]
            j+=1
          All_out.append(wholeData)
        # general code for generating YES-k bounds
        if Layers != 1:
          layer_indces=tc.arange(0,Layers-1)
          for k in range(1,Layers):
            layer_combinations=tc.combinations(layer_indces, r=k)
            temp_k_bound=tc.zeros(layer_combinations.shape[0], dtype = tc.float32)
            for i in range(layer_combinations.shape[0]):
              l=0
              Y_sigma=[]
              for j in range(k):
                Y_sigma.append(All_out[layer_combinations[i][j].item()].transpose(0,1))
              Y_k=X
              for j in range(k):
                while l<=layer_combinations[i][j].item():
                  if not bias_cond:
                    # with no bias
                    W_k=tc.matmul(Y_sigma[j],tc.linalg.pinv(Y_k))
                    Y_k=tc.nn.functional.relu(tc.matmul(W_k,Y_k))
                  else:
                    # with bias
                    Y_t=tc.vstack((Y_k, tc.ones(1,Num_train)))
                    W_k=tc.matmul(Y_sigma[j],tc.linalg.pinv(Y_t))
                    Y_k=tc.nn.functional.relu(tc.matmul(W_k,Y_t))
                  l+=1
              for cnt in range(Layers-l):
                if not bias_cond:
                  # with no bias
                  W_k=tc.matmul(Y,tc.linalg.pinv(Y_k))
                  if cnt != Layers-l-1:
                    Y_k=tc.nn.functional.relu(tc.matmul(W_k,Y_k))
                  else:
                    Y_k=tc.matmul(W_k,Y_k)
                    #Y_k=tc.nn.functional.relu(tc.matmul(W_k,Y_k))
                else:
                  # with bias
                  Y_t=tc.vstack((Y_k, tc.ones(1,Num_train)))
                  W_k=tc.matmul(Y,tc.linalg.pinv(Y_t))
                  if cnt != Layers-l-1:
                    Y_k=tc.nn.functional.relu(tc.matmul(W_k,Y_t))
                  else:
                    Y_k=tc.matmul(W_k,Y_t)
                    #Y_k=tc.nn.functional.relu(tc.matmul(W_k,Y_t))
              s=0
              for h in range(Num_train):
                s+=(tc.norm(Y_k[:,h]-Y[:,h])**2)
              s=s/Num_train
              temp_k_bound[i]=s
            #YES_k_bounds[k-1,epoch]=tc.min(temp_k_bound)
            if k==1:
              YES_k_bounds[k-1,epoch]=tc.min(tc.cat((YES_0_bound,temp_k_bound)))
              if (epoch>=1) and (YES_k_bounds[k-1,epoch]-YES_k_bounds[k-1,epoch-1]>0):
                YES_k_bounds[k-1,epoch]=YES_k_bounds[k-1,epoch-1]
            else:
              YES_k_bounds[k-1,epoch]=tc.min(tc.cat((temp_k_bound,tc.tensor([YES_k_bounds[k-2,epoch].item()]))))
              if (epoch>=1) and (YES_k_bounds[k-1,epoch]-YES_k_bounds[k-1,epoch-1]>0):
                YES_k_bounds[k-1,epoch]=YES_k_bounds[k-1,epoch-1]
      if epoch%1==0:
        with tc.no_grad():
          train_loss=0
          for batch in train_loader:
            x,b=batch
            x=x.to(tc.float32)
            b=b.to(tc.float32)
            b_hat,_=model(x)
            train_loss+=mse(b_hat,b)
          train_loss=train_loss/Num_train
          train_loss_ep.append(train_loss.item())
          test_loss=0
          for batch in test_loader:
            x,b=batch
            x=x.to(tc.float32)
            b=b.to(tc.float32)
            b_hat,_=model(x)
            test_loss+=mse(b_hat,b)
          test_loss=test_loss/Num_test
          test_loss_ep.append(test_loss.item())
          print('Epoch {}/{}'.format(epoch, NUM_EPOCHS),'Train LOSS: {:.2f}'.format(train_loss.item()),'|| {:.2f}'.format(10*np.log10(train_loss.item())),'dB\n',\
                'test LOSS: {:.2f}'.format(test_loss.item()),'|| {:.2f}'.format(10*np.log10(test_loss.item())),'dB\n')
      # plot online
      YES_3_bound=YES_k_bounds[Layers-2,0:epoch+1]
      clear_output(wait=True)
      fig, ax = plt.subplots()
      # Plot the data with different colors and line styles
      plt.plot([10*np.log10(YES_0_bound.item())] * len(YES_3_bound), color='red', linestyle='-', linewidth=2)
      plt.plot(10*np.log10(YES_3_bound), color='orange', linestyle='-.', linewidth=2)
      plt.plot(10*np.log10(train_loss_ep), color='navy', linestyle='-', linewidth=2, label='Training Loss')
      #for i in range(len(YES_3_bound)):
      #  if train_loss_ep[i] < YES_3_bound[i]:
      #    plt.scatter(i, 10*np.log10(train_loss_ep[i]), color='red', marker='o', s=20)
      #
      ymin, ymax = ax.get_ylim()
      # Set the background color regions using fill_between for non-straight regions
      plt.fill_between(np.arange(0, len(YES_3_bound), 1), 10*np.log10(YES_0_bound.item()), ymax, facecolor=pleasant_red, alpha=0.4, hatch='xx', edgecolor='red', label='Ineffective Training')
      plt.fill_between(np.arange(0, len(YES_3_bound), 1), 10*np.log10(YES_3_bound), 10*np.log10(YES_0_bound.item()), facecolor=golden_color, alpha=0.4, hatch='//', edgecolor='gold', label='Caution')
      plt.fill_between(np.arange(0, len(YES_3_bound), 1), ymin, 10*np.log10(YES_3_bound), facecolor=spring_green, alpha=0.4, label='Effective Training')
      plt.ylabel('MSE(dB)')
      plt.xlabel('Epoch number')
      plt.legend(loc='center', bbox_to_anchor=(0.85, 0.8), fontsize='small', borderpad=0.2, labelspacing=0.3)
      plt.grid(alpha=0.7)
      plt.show()
   return  train_loss_ep,test_loss_ep,YES_0_bound,YES_k_bounds,YES_0_layer
############################################################################################################################################################
def mse(b, b_star):
  #norm_b_star = tc.norm(b_star, dim=1)**2
  temp=tc.sum((b - b_star)**2,dim=1)
  mse=tc.sum(temp)
  return mse

# set random seeds for reproducibility
params=struct()
#tc.manual_seed(42)
#np.random.seed(42)
def check_vector_type(vector):
    if isinstance(vector, np.ndarray):
        return "NumPy array"
    elif isinstance(vector, tc.Tensor):
        return "PyTorch tensor"
    else:
        return "Unknown type"
def relu(x):
    return np.maximum(0, x)
params.m=100
params.n=20
params.whole_dataset_size=1000
m=params.m
n=params.n
whole_dataset_size=params.whole_dataset_size
x = np.zeros((whole_dataset_size, n))
b = np.zeros((whole_dataset_size, m))
A_fix = np.random.normal(0, np.sqrt(1/m), (m, n))
for i in range(whole_dataset_size):
  x[i,:]=np.random.normal(0, 1, (n, 1)).squeeze(1)
  b[i,:]=np.abs(np.matmul(A_fix,x[i,:]))
  #b[i,:]=relu(np.matmul(A_fix,x[i,:]))
  x[i,:]=x[i,:].astype(float)
  b[i,:]=b[i,:].astype(float)
params.x=x
params.b=b

params.Layers      = 5
params.BATCH_SIZE  = 50
params.NUM_EPOCHS  = 800
params.lr          = 1e-3  # 1e-3 is  best
params.split       = 0.2
params.bias        = True

tc.cuda.empty_cache()
params.x_train, params.x_test, params.b_train, params.b_test = train_test_split(params.x, params.b, test_size=params.split, random_state=2)
train_loader,test_loader=prepare_train_test(params)

split=params.split
Num_train=(1-split)*whole_dataset_size
Num_train=int(Num_train)
Y=tc.zeros((Num_train,m), dtype = tc.float32)
X=tc.zeros((Num_train,n), dtype = tc.float32)
BATCH_SIZE=params.BATCH_SIZE
j=0
for batch in train_loader:
  x,b=batch
  b=b.to(tc.float32)
  x=x.to(tc.float32)
  X[BATCH_SIZE*j:BATCH_SIZE*(j+1),:]=x
  Y[BATCH_SIZE*j:BATCH_SIZE*(j+1),:]=b
  j+=1
Y=Y.transpose(0,1)
X=X.transpose(0,1)
params.Ydata=Y
params.Xdata=X

model=Fully_model(params)
params.train_loss,params.test_loss,params.YES_0_bound,params.YES_k_bounds,params.YES_0_layer = train(model,params,train_loader,test_loader)

YES_0_bound=params.YES_0_bound
YES_3_bound=params.YES_k_bounds[params.Layers-2,:]

train_loss=params.train_loss
fig, ax = plt.subplots()
# Define colors
spring_green = "#00FF7F"  # Bright and vibrant green
golden_color = "#FFD700"  # Golden color for the yellow area
pleasant_red = "#FFB6C1"  # Light pinkish red for the red area
# Plot the data with different colors and line styles
plt.plot(np.arange(0, len(YES_3_bound), 1), [10*np.log10(YES_0_bound.item())] * len(YES_3_bound),
         color='red', linestyle='-', linewidth=2, label='YES-0 Bound')
plt.plot(np.arange(0, len(YES_3_bound), 1), 10*np.log10(YES_3_bound),
         color='green', linestyle='--', linewidth=2, label='YES-4 Bound')
plt.plot(np.arange(0, len(YES_3_bound), 1), 10*np.log10(train_loss),
         color='navy', linestyle='-', linewidth=2, label='Training Loss')
# get the y-axis limits
ymin, ymax = ax.get_ylim()
# Set the background color regions using fill_between for non-straight regions
plt.fill_between(np.arange(0, len(YES_3_bound), 1), 10*np.log10(YES_0_bound.item()), ymax, facecolor=pleasant_red, alpha=0.4, hatch='xx', edgecolor='red', label='Ineffective Training')
plt.fill_between(np.arange(0, len(YES_3_bound), 1), 10*np.log10(YES_3_bound), 10*np.log10(YES_0_bound.item()), facecolor=golden_color, alpha=0.4, hatch='//', edgecolor='gold', label='YES Cloud (Caution)')
plt.fill_between(np.arange(0, len(YES_3_bound), 1), ymin, 10*np.log10(YES_3_bound), facecolor=spring_green, alpha=0.4, label='Effective Training')
###
plt.tight_layout()
plt.ylabel('MSE(dB)',fontsize=13)
plt.xlabel('Epoch Number',fontsize=13)
plt.title('YES Cloud and Training Loss Progression',fontsize=15)
plt.legend(loc='center', bbox_to_anchor=(0.825, 0.8), fontsize='small', borderpad=0.2, labelspacing=0.3)
plt.grid(alpha=0.7)
plt.savefig("/content/gdrive/My Drive/ICLR_2025_YES_bound_paper/pr_1_1.png", format="png", bbox_inches="tight")
#time.sleep(10)

# creating YES cloud for the paper
train_loss=params.train_loss
fig, ax = plt.subplots()
# Define colors
spring_green = "#00FF7F"  # Bright and vibrant green
golden_color = "#FFD700"  # Golden color for the yellow area
pleasant_red = "#FFB6C1"  # Light pinkish red for the red area
# Plot the data with different colors and line styles
# get the y-axis limits
ymin, ymax = ax.get_ylim()
# Set the background color regions using fill_between for non-straight regions
plt.fill_between(np.arange(0, len(YES_3_bound), 1), 10*np.log10(YES_0_bound.item()), 13, facecolor=pleasant_red, alpha=0.4, hatch='xx', edgecolor='red', label='Ineffective Training')
plt.fill_between(np.arange(0, len(YES_3_bound), 1), 10*np.log10(YES_3_bound), 10*np.log10(YES_0_bound.item()), facecolor=golden_color, alpha=0.4, hatch='//', edgecolor='gold', label='Caution')
plt.fill_between(np.arange(0, len(YES_3_bound), 1), 0, 10*np.log10(YES_3_bound), facecolor=spring_green, alpha=0.4, label='Effective Training')
###
plt.tight_layout()
plt.ylabel('MSE(dB)',fontsize=13)
plt.xlabel('Epoch Number',fontsize=13)
plt.title('Color-Coded YES Cloud',fontsize=15)
#plt.legend(loc='center', bbox_to_anchor=(0.825, 0.8), fontsize='small', borderpad=0.2, labelspacing=0.3)
plt.grid(alpha=0.7)
#plt.savefig("/content/gdrive/My Drive/ICLR_2025_YES_bound_paper/pr_plateau.png", format="png", bbox_inches="tight")

# plot for YES_0_bound perlayer
YES_0_layer = params.YES_0_layer
Layers=params.Layers
YES_0_error = tc.zeros((Layers), dtype = tc.float32)
for i in range(Layers):
  s=0
  for j in range(Num_train):
    s+=(tc.norm(YES_0_layer[i][:,j]-Y[:,j])**2)
  s=s/Num_train
  YES_0_error[i]=s
print(YES_0_error)

fig0, ax0 = plt.subplots()
plt.plot(np.arange(1, Layers+1, 1), 10*np.log10(YES_0_error), color='red', linestyle='-', marker='o', linewidth=2, label='Per-Layer YES-0 Bound')
plt.tight_layout()
plt.xlabel('Layer')
plt.ylabel('MSE (dB)')
plt.grid(True)
plt.legend(loc='center', bbox_to_anchor=(0.75, 0.9), borderpad=0.2, labelspacing=0.3)
#plt.savefig("/content/gdrive/My Drive/ICLR_2025_YES_bound_paper/YES_0_layer.png", format="png", bbox_inches="tight")

fig, ax = plt.subplots()
# Plot the data with different colors and line styles
#plt.plot([10*np.log10(YES_0_bound.item())] * len(YES_3_bound), color='red', linestyle='-', linewidth=2)
plt.plot(10*np.log10(YES_1_bound), color='orange', linestyle='-', linewidth=2, label='YES-1 bound')
plt.plot(10*np.log10(YES_2_bound), color='brown', linestyle='--', linewidth=2, label='YES-2 bound')
plt.plot(10*np.log10(YES_3_bound), color='cyan', linestyle='-.', linewidth=2, label='YES-3 bound')
plt.plot(10*np.log10(YES_4_bound), color='magenta', linestyle=':', linewidth=2, label='YES-4 bound')
###
plt.tight_layout()
plt.ylabel('MSE(dB)',fontsize=13)
plt.xlabel('Epoch Number',fontsize=13)
plt.title('YES Bounds',fontsize=15)
plt.legend(loc='center', bbox_to_anchor=(0.825, 0.78), fontsize='small', borderpad=0.2, labelspacing=0.3)
plt.grid(alpha=0.7)
#plt.savefig("/content/gdrive/My Drive/ICLR_2025_YES_bound_paper/YES_bounds_close_3.png", format="png", bbox_inches="tight")