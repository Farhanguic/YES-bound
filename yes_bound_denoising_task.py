# -*- coding: utf-8 -*-
"""YES_bound_denoising_task.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Etu7sMYQb_aZFURbDMMC5boMVMi5rix4
"""

import numpy as np
import torch as tc
import torch.nn as nn
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
import torch.optim.lr_scheduler as lr_scheduler
from torch.optim.lr_scheduler import MultiStepLR
import torch.nn.functional as F
from google.colab import drive
import pickle
import copy
import time
import torch.nn.init as init
from IPython.display import clear_output
# Mount Google Drive
drive.mount('/content/gdrive')

class struct():
    '''
    an empty class to use structure type variable
    '''
    pass

class Fully_model(tc.nn.Module):
    def __init__(self,params):
      super(Fully_model, self).__init__()
      self.K=params.Layers
      m=params.m
      Layers=params.Layers
      self.W=nn.ModuleList([nn.Linear(m,m,bias=params.bias) for _ in range(Layers)])
############################################################################################################################################################
    def forward(self,x):
      per_out=[]
      x_k=x.to(tc.float32)
      for k in range(self.K):
        temp=self.W[k](x_k)
        #x_k=tc.nn.functional.relu(temp)
        if k != self.K-1:
          x_k=tc.nn.functional.relu(temp)
        else:
          x_k=temp
        per_out.append(x_k)
      return  x_k,per_out
############################################################################################################################################################
def  prepare_train_test(params):
    x_train=params.x_train
    x_test=params.x_test
    BATCH_SIZE=params.BATCH_SIZE
    b_train=params.b_train
    b_test=params.b_test
    # convert data to  iterables
    trainset = []
    for i in range(len(x_train)):
      trainset.append([x_train[i], b_train[i]])
    train_loader= DataLoader(trainset, shuffle=False, batch_size=BATCH_SIZE)
    testset = []
    for i in range(len(x_test)):
      testset.append([x_test[i], b_test[i]])
    test_loader= DataLoader(testset, shuffle=False, batch_size=BATCH_SIZE)
    return train_loader,test_loader
############################################################################################################################################################
def train(model,params,train_loader,test_loader):
   whole_dataset_size=params.whole_dataset_size
   split=params.split
   Num_train=(1-split)*whole_dataset_size
   Num_train=int(Num_train)
   Num_test=(split)*whole_dataset_size
   Num_test=int(Num_test)
   m=params.m
   BATCH_SIZE=params.BATCH_SIZE
   NUM_EPOCHS=params.NUM_EPOCHS
   Layers=params.Layers
   X=params.Xdata
   Y=params.Ydata
   bias_cond=params.bias
   YES_0_layer=[]
   # computing YES-0 bound for training
   if not bias_cond:
    # with no bias
    W_k=tc.matmul(Y,tc.linalg.pinv(X))
    Y_k=tc.nn.functional.relu(tc.matmul(W_k,X))
    YES_0_layer.append(Y_k)
    for k in range(Layers-1):
      W_k=tc.matmul(Y,tc.linalg.pinv(Y_k))
      if k != Layers-2:
        Y_k=tc.nn.functional.relu(tc.matmul(W_k,Y_k))
        YES_0_layer.append(Y_k)
      else:
        Y_k=tc.matmul(W_k,Y_k)
        #Y_k=tc.nn.functional.relu(tc.matmul(W_k,Y_k))
        YES_0_layer.append(Y_k)
   else:
    # with bias
    Y_k=X
    for k in range(Layers):
      Y_t=tc.vstack((Y_k, tc.ones(1,Num_train)))
      W_k=tc.matmul(Y,tc.linalg.pinv(Y_t))
      if k != Layers-1:
        Y_k=tc.nn.functional.relu(tc.matmul(W_k,Y_t))
        YES_0_layer.append(Y_k)
      else:
        Y_k=tc.matmul(W_k,Y_t)
        #Y_k=tc.nn.functional.relu(tc.matmul(W_k,Y_t))
        YES_0_layer.append(Y_k)
   s=0
   for i in range(Num_train):
    s+=(tc.norm(Y_k[:,i]-Y[:,i])**2)
   s=s/Num_train
   YES_0_bound=tc.tensor([s])
   #report=struct()
   criterion=tc.nn.MSELoss(reduction='sum') # square error loss
   optimizer=tc.optim.SGD(model.parameters(),lr=params.lr)
   scheduler=lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.9)
   train_loss_ep=[]
   test_loss_ep=[]
   YES_k_bounds=tc.zeros((Layers-1,NUM_EPOCHS), dtype = tc.float32)
   # Define colors
   spring_green = "#00FF7F"  # Bright and vibrant green
   golden_color = "#FFD700"   # Golden color for the yellow area
   pleasant_red = "#FFB6C1"   # Light pinkish red for the red area
   # determine whether bias of l
   bias_cond=params.bias
   for epoch in range(NUM_EPOCHS):
      for batch in train_loader:
           x,b=batch
           x=x.to(tc.float32)
           b=b.to(tc.float32)
           # forward pass
           x_hat,_=model(b)
           loss=criterion(x_hat,x) # our objective function
           # backward pass
           optimizer.zero_grad()
           loss.backward()
           # update
           optimizer.step()
      scheduler.step()
      current_lr = optimizer.param_groups[0]['lr']
      print(f"Epoch {epoch}, Learning Rate: {current_lr}")
      # creating the per-layer output
      with tc.no_grad():
        All_out=[]
        for i in range(Layers):
          wholeData=tc.zeros((Num_train,m), dtype = tc.float32)
          j=0
          for batch in train_loader:
            _,b=batch
            b=b.to(tc.float32)
            _,per_out=model(b)
            wholeData[BATCH_SIZE*j:BATCH_SIZE*(j+1),:]=per_out[i]
            j+=1
          All_out.append(wholeData)
        # general code for generating YES-k bounds
        if Layers != 1:
          layer_indces=tc.arange(0,Layers-1)
          for k in range(1,Layers):
            layer_combinations=tc.combinations(layer_indces, r=k)
            temp_k_bound=tc.zeros(layer_combinations.shape[0], dtype = tc.float32)
            for i in range(layer_combinations.shape[0]):
              l=0
              Y_sigma=[]
              for j in range(k):
                Y_sigma.append(All_out[layer_combinations[i][j].item()].transpose(0,1))
              Y_k=X
              for j in range(k):
                while l<=layer_combinations[i][j].item():
                  if not bias_cond:
                    # with no bias
                    W_k=tc.matmul(Y_sigma[j],tc.linalg.pinv(Y_k))
                    Y_k=tc.nn.functional.relu(tc.matmul(W_k,Y_k))
                  else:
                    # with bias
                    Y_t=tc.vstack((Y_k, tc.ones(1,Num_train)))
                    W_k=tc.matmul(Y_sigma[j],tc.linalg.pinv(Y_t))
                    Y_k=tc.nn.functional.relu(tc.matmul(W_k,Y_t))
                  l+=1
              for cnt in range(Layers-l):
                if not bias_cond:
                  # with no bias
                  W_k=tc.matmul(Y,tc.linalg.pinv(Y_k))
                  if cnt != Layers-l-1:
                    Y_k=tc.nn.functional.relu(tc.matmul(W_k,Y_k))
                  else:
                    Y_k=tc.matmul(W_k,Y_k)
                    #Y_k=tc.nn.functional.relu(tc.matmul(W_k,Y_k))
                else:
                  # with bias
                  Y_t=tc.vstack((Y_k, tc.ones(1,Num_train)))
                  W_k=tc.matmul(Y,tc.linalg.pinv(Y_t))
                  if cnt != Layers-l-1:
                    Y_k=tc.nn.functional.relu(tc.matmul(W_k,Y_t))
                  else:
                    Y_k=tc.matmul(W_k,Y_t)
                    #Y_k=tc.nn.functional.relu(tc.matmul(W_k,Y_t))
              s=0
              for h in range(Num_train):
                s+=(tc.norm(Y_k[:,h]-Y[:,h])**2)
              s=s/Num_train
              temp_k_bound[i]=s
            #YES_k_bounds[k-1,epoch] = tc.min(temp_k_bound)
            if k==1:
              YES_k_bounds[k-1,epoch]=tc.min(tc.cat((YES_0_bound,temp_k_bound)))
              if (epoch>=1) and (YES_k_bounds[k-1,epoch]-YES_k_bounds[k-1,epoch-1]>0):
                YES_k_bounds[k-1,epoch]=YES_k_bounds[k-1,epoch-1]
            else:
              YES_k_bounds[k-1,epoch]=tc.min(tc.cat((temp_k_bound,tc.tensor([YES_k_bounds[k-2,epoch].item()]))))
              if (epoch>=1) and (YES_k_bounds[k-1,epoch]-YES_k_bounds[k-1,epoch-1]>0):
                YES_k_bounds[k-1,epoch]=YES_k_bounds[k-1,epoch-1]
      if epoch%1==0:
        with tc.no_grad():
          train_loss=0
          for batch in train_loader:
            x,b=batch
            x=x.to(tc.float32)
            b=b.to(tc.float32)
            x_hat,_=model(b)
            train_loss+=mse(x_hat,x)
          train_loss=train_loss/Num_train
          train_loss_ep.append(train_loss.item())
          test_loss=0
          for batch in test_loader:
            x,b=batch
            x=x.to(tc.float32)
            b=b.to(tc.float32)
            x_hat,_=model(b)
            test_loss+=mse(x_hat,x)
          test_loss=test_loss/Num_test
          test_loss_ep.append(test_loss.item())
          print('Epoch {}/{}'.format(epoch, NUM_EPOCHS),'Train LOSS: {:.2f}'.format(train_loss.item()),'|| {:.2f}'.format(10*np.log10(train_loss.item())),'dB\n',\
                'test LOSS: {:.2f}'.format(test_loss.item()),'|| {:.2f}'.format(10*np.log10(test_loss.item())),'dB\n')
      YES_3_bound=YES_k_bounds[Layers-2,0:epoch+1]
      clear_output(wait=True)
      fig, ax = plt.subplots()
      # Plot the data with different colors and line styles
      plt.plot([10*np.log10(YES_0_bound.item())] * len(YES_3_bound), color='red', linestyle='-', linewidth=2)
      plt.plot(10*np.log10(YES_3_bound), color='orange', linestyle='-.', linewidth=2)
      plt.plot(10*np.log10(train_loss_ep), color='navy', linestyle='-', linewidth=2, label='Training Loss')
      #for i in range(len(YES_3_bound)):
      #  if train_loss_ep[i] < YES_3_bound[i]:
      #    plt.scatter(i, 10*np.log10(train_loss_ep[i]), color='red', marker='o', s=20)
      #
      ymin, ymax = ax.get_ylim()
      # Set the background color regions using fill_between for non-straight regions
      plt.fill_between(np.arange(0, len(YES_3_bound), 1), 10*np.log10(YES_0_bound.item()), ymax, facecolor=pleasant_red, alpha=0.4, hatch='xx', edgecolor='red', label='Ineffective Training')
      plt.fill_between(np.arange(0, len(YES_3_bound), 1), 10*np.log10(YES_3_bound), 10*np.log10(YES_0_bound.item()), facecolor=golden_color, alpha=0.4, hatch='//', edgecolor='gold', label='Caution')
      plt.fill_between(np.arange(0, len(YES_3_bound), 1), ymin, 10*np.log10(YES_3_bound), facecolor=spring_green, alpha=0.4, label='Effective Training')
      plt.ylabel('MSE(dB)')
      plt.xlabel('Epoch number')
      #plt.legend(loc='center', bbox_to_anchor=(0.75, 0.85))
      plt.legend(loc='center', bbox_to_anchor=(0.85, 0.8), fontsize='small', borderpad=0.2, labelspacing=0.3)
      plt.grid(alpha=0.7)
      plt.show()
   return  train_loss_ep,test_loss_ep,YES_0_bound,YES_k_bounds,YES_0_layer
############################################################################################################################################################
def mse(b, b_star):
  #norm_b_star = tc.norm(b_star, dim=1)**2
  temp=tc.sum((b - b_star)**2,dim=1)
  mse=tc.sum(temp)
  return mse

# set random seeds for reproducibility
params=struct()
#tc.manual_seed(42)
#np.random.seed(42)
def check_vector_type(vector):
    if isinstance(vector, np.ndarray):
        return "NumPy array"
    elif isinstance(vector, tc.Tensor):
        return "PyTorch tensor"
    else:
        return "Unknown type"
def relu(x):
    return np.maximum(0, x)
n_fix=50
params.m=20
params.whole_dataset_size=1000
m=params.m
whole_dataset_size=params.whole_dataset_size
x=np.zeros((whole_dataset_size, m))
b=np.zeros((whole_dataset_size, m))
x_fix=np.zeros((n_fix, m))
for i in range(n_fix):
  x_fix[i]=np.random.normal(0, 1, (m, 1)).squeeze(1)
num_noise=whole_dataset_size/n_fix
num_noise=int(num_noise)
i=0
j=0
while i<whole_dataset_size:
  for _ in range(num_noise):
    x[i]=x_fix[j]
    n=np.random.normal(0, 0.2, (m, 1)).squeeze(1)
    b[i]=x[i]+n
    x[i]=x[i].astype(float)
    b[i]=b[i].astype(float)
    i+=1
  j+=1
params.x=x
params.b=b

params.Layers      = 5
params.BATCH_SIZE  = 20
params.NUM_EPOCHS  = 1800
params.lr          = 1e-4   # 1e-3 is  best
params.split       = 0.2
params.bias        = True

tc.cuda.empty_cache()
params.x_train, params.x_test, params.b_train, params.b_test = train_test_split(params.x, params.b, test_size=params.split, random_state=2)
train_loader,test_loader=prepare_train_test(params)

split=params.split
Num_train=(1-split)*whole_dataset_size
Num_train=int(Num_train)
Y=tc.zeros((Num_train,m), dtype = tc.float32)
X=tc.zeros((Num_train,m), dtype = tc.float32)
BATCH_SIZE=params.BATCH_SIZE
j=0
for batch in train_loader:
  x,b=batch
  b=b.to(tc.float32)
  x=x.to(tc.float32)
  X[BATCH_SIZE*j:BATCH_SIZE*(j+1),:]=b
  Y[BATCH_SIZE*j:BATCH_SIZE*(j+1),:]=x
  j+=1
Y=Y.transpose(0,1)
X=X.transpose(0,1)
params.Ydata=Y
params.Xdata=X

model=Fully_model(params)
params.train_loss,params.test_loss,params.YES_0_bound,params.YES_k_bounds,params.YES_0_layer = train(model,params,train_loader,test_loader)
#print(perLayerError)

YES_0_bound=params.YES_0_bound
YES_3_bound=params.YES_k_bounds[params.Layers-2,:]

train_loss=params.train_loss
fig, ax = plt.subplots()
# Define colors
spring_green = "#00FF7F"  # Bright and vibrant green
golden_color = "#FFD700"  # Golden color for the yellow area
pleasant_red = "#FFB6C1"  # Light pinkish red for the red area
# Plot the data with different colors and line styles
plt.plot(np.arange(0, len(YES_3_bound), 1), [10*np.log10(YES_0_bound.item())] * len(YES_3_bound),
         color='red', linestyle='-', linewidth=2, label='YES-0 Bound')
plt.plot(np.arange(0, len(YES_3_bound), 1), 10*np.log10(YES_3_bound),
         color='green', linestyle='--', linewidth=2, label='YES-4 Bound')
plt.plot(np.arange(0, len(YES_3_bound), 1), 10*np.log10(train_loss),
         color='navy', linestyle='-', linewidth=2, label='Training Loss')
# get the y-axis limits
ymin, ymax = ax.get_ylim()
# Set the background color regions using fill_between for non-straight regions
plt.fill_between(np.arange(0, len(YES_3_bound), 1), 10*np.log10(YES_0_bound.item()), ymax, facecolor=pleasant_red, alpha=0.4, hatch='xx', edgecolor='red', label='Ineffective Training')
plt.fill_between(np.arange(0, len(YES_3_bound), 1), 10*np.log10(YES_3_bound), 10*np.log10(YES_0_bound.item()), facecolor=golden_color, alpha=0.4, hatch='//', edgecolor='gold', label='YES Cloud (Caution)')
plt.fill_between(np.arange(0, len(YES_3_bound), 1), ymin, 10*np.log10(YES_3_bound), facecolor=spring_green, alpha=0.4, label='Effective Training')
###
plt.tight_layout()
plt.ylabel('MSE(dB)',fontsize=13)
plt.xlabel('Epoch Number',fontsize=13)
plt.title('YES Cloud and Training Loss Progression',fontsize=15)
plt.legend(loc='center', bbox_to_anchor=(0.825, 0.78), fontsize='small', borderpad=0.2, labelspacing=0.3)
plt.grid(alpha=0.7)
plt.savefig("/content/gdrive/My Drive/ICLR_2025_YES_bound_paper/signal_denoising_layer_5.png", format="png", bbox_inches="tight")
#time.sleep(10)